#!/usr/bin/env tsx
/**
 * LLM 8-model comparison runner.
 *
 * 8ê°œ ëª¨ë¸ì— ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ë³´ë‚´ê³  í’ˆì§ˆ/ë¹„ìš©/ì†ë„ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
 * ìµœì í™”: JSON mode, í”„ë¡¬í”„íŠ¸ ìºì‹± ì ìš©.
 *
 * Usage:
 *   pnpm tsx scripts/compare-llm.ts
 *
 * í•„ìš” í™˜ê²½ë³€ìˆ˜: OPENAI_API_KEY, ANTHROPIC_API_KEY, GOOGLE_API_KEY
 * (.env íŒŒì¼ì—ì„œ ìë™ ë¡œë“œ)
 */

import { config } from "dotenv";
import { resolve } from "path";

// Load .env â€” try multiple paths to find the root .env
const scriptDir = import.meta.dirname ?? __dirname;
const candidates = [
  resolve(scriptDir, "../.env"),
  resolve(process.cwd(), ".env"),
  resolve(process.cwd(), "../../.env"),
];
for (const p of candidates) {
  config({ path: p, override: true });
}

// â”€â”€ Model definitions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

type Provider = "openai" | "anthropic" | "google";

interface ModelDef {
  id: string;
  provider: Provider;
  modelName: string;
  costPer1kInput: number;  // USD per 1K input tokens
  costPer1kOutput: number; // USD per 1K output tokens
}

const MODELS: ModelDef[] = [
  // OpenAI â€” GPT-5 + GPT-4.1 (prices per 1K tokens = per 1M / 1000)
  { id: "gpt-5-mini",    provider: "openai",    modelName: "gpt-5-mini",    costPer1kInput: 0.00025,  costPer1kOutput: 0.002 },
  { id: "gpt-5-nano",    provider: "openai",    modelName: "gpt-5-nano",    costPer1kInput: 0.00005,  costPer1kOutput: 0.0004 },
  { id: "gpt-4.1-mini",  provider: "openai",    modelName: "gpt-4.1-mini",  costPer1kInput: 0.0004,   costPer1kOutput: 0.0016 },
  { id: "gpt-4.1-nano",  provider: "openai",    modelName: "gpt-4.1-nano",  costPer1kInput: 0.0001,   costPer1kOutput: 0.0004 },
  // Anthropic â€” Claude 4.6 / 4.5
  { id: "claude-sonnet-4-6",  provider: "anthropic", modelName: "claude-sonnet-4-6",         costPer1kInput: 0.003,  costPer1kOutput: 0.015 },
  { id: "claude-haiku-4-5",   provider: "anthropic", modelName: "claude-haiku-4-5-20251001",  costPer1kInput: 0.001,  costPer1kOutput: 0.005 },
  // Google â€” Gemini 2.5 stable (2.0 deprecated Jun 2026)
  { id: "gemini-2.5-flash",      provider: "google", modelName: "gemini-2.5-flash",       costPer1kInput: 0.00015, costPer1kOutput: 0.0006 },
  { id: "gemini-2.5-flash-lite", provider: "google", modelName: "gemini-2.5-flash-lite",  costPer1kInput: 0.0001,  costPer1kOutput: 0.0004 },
];

// â”€â”€ Provider callers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

type CallResult = {
  text: string;
  inputTokens: number;
  outputTokens: number;
  cacheHit?: boolean;
};

async function callOpenAI(modelName: string, system: string, user: string, maxTokens: number): Promise<CallResult> {
  // GPT-5+ uses max_completion_tokens and temperature=1 only
  const isGpt5 = modelName.startsWith("gpt-5");
  const tokenParam = isGpt5
    ? { max_completion_tokens: maxTokens }
    : { max_tokens: maxTokens };

  const resp = await fetch("https://api.openai.com/v1/chat/completions", {
    method: "POST",
    headers: { "Content-Type": "application/json", Authorization: `Bearer ${process.env.OPENAI_API_KEY}` },
    body: JSON.stringify({
      model: modelName, temperature: isGpt5 ? 1 : 0.7, ...tokenParam,
      // JSON mode â€” forces valid JSON, saves tokens from markdown wrapping
      response_format: { type: "json_object" },
      messages: [{ role: "system", content: system }, { role: "user", content: user }],
    }),
  });
  if (!resp.ok) throw new Error(`OpenAI ${modelName}: ${resp.status} ${await resp.text()}`);
  const json = await resp.json() as any;
  return {
    text: json.choices[0].message.content,
    inputTokens: json.usage?.prompt_tokens ?? 0,
    outputTokens: json.usage?.completion_tokens ?? 0,
  };
}

async function callAnthropic(modelName: string, system: string, user: string, maxTokens: number): Promise<CallResult> {
  const resp = await fetch("https://api.anthropic.com/v1/messages", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      "x-api-key": process.env.ANTHROPIC_API_KEY!,
      "anthropic-version": "2023-06-01",
      // Enable prompt caching â€” caches system prompt across requests
      "anthropic-beta": "prompt-caching-2024-07-31",
    },
    body: JSON.stringify({
      model: modelName,
      max_tokens: maxTokens,
      temperature: 0.7,
      // Prompt caching: mark system prompt as cacheable
      system: [
        {
          type: "text",
          text: system,
          cache_control: { type: "ephemeral" },
        }
      ],
      messages: [{ role: "user", content: user }],
    }),
  });
  if (!resp.ok) throw new Error(`Anthropic ${modelName}: ${resp.status} ${await resp.text()}`);
  const json = await resp.json() as any;
  const cacheRead = json.usage?.cache_read_input_tokens ?? 0;
  return {
    text: json.content[0].text,
    inputTokens: json.usage?.input_tokens ?? 0,
    outputTokens: json.usage?.output_tokens ?? 0,
    cacheHit: cacheRead > 0,
  };
}

async function callGoogle(modelName: string, system: string, user: string, maxTokens: number): Promise<CallResult> {
  const apiKey = process.env.GOOGLE_API_KEY;
  const resp = await fetch(
    `https://generativelanguage.googleapis.com/v1beta/models/${modelName}:generateContent?key=${apiKey}`,
    {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        systemInstruction: { parts: [{ text: system }] },
        contents: [{ role: "user", parts: [{ text: user }] }],
        generationConfig: {
          temperature: 0.7,
          maxOutputTokens: maxTokens,
          // JSON mode â€” forces valid JSON output
          responseMimeType: "application/json",
        },
      }),
    }
  );
  if (!resp.ok) throw new Error(`Gemini ${modelName}: ${resp.status} ${await resp.text()}`);
  const json = await resp.json() as any;
  const meta = json.usageMetadata;
  return {
    text: json.candidates[0].content.parts[0].text,
    inputTokens: meta?.promptTokenCount ?? 0,
    outputTokens: meta?.candidatesTokenCount ?? 0,
  };
}

async function callModel(def: ModelDef, system: string, user: string, maxTokens: number): Promise<CallResult> {
  if (def.provider === "openai") return callOpenAI(def.modelName, system, user, maxTokens);
  if (def.provider === "anthropic") return callAnthropic(def.modelName, system, user, maxTokens);
  return callGoogle(def.modelName, system, user, maxTokens);
}

// â”€â”€ Test prompt (ê°„ë‹¨í•œ ë¬´ë£Œ í”„ë¦¬ë·°ìš©) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

const SYSTEM = `ì—­í• : í•œêµ­ ì‚¬ì£¼(å››æŸ±) ì „ë¬¸ ë¦¬í¬íŠ¸ ì‘ì„±ì.
ê·œì¹™: ì¡´ëŒ“ë§, í™•ë¥  í‘œí˜„ë§Œ ì‚¬ìš©, ë‹¨ì • ê¸ˆì§€, ê³µí¬ ê¸ˆì§€.
ì¶œë ¥: JSONë§Œ ì¶œë ¥ (ì¶”ê°€ í…ìŠ¤íŠ¸/ë§ˆí¬ë‹¤ìš´ ê¸ˆì§€)
{"headline":string,"summary":string,"sections":[{"key":"ì„±ê²©","title":"ì„±ê²©","text":string},{"key":"ì§ì—…","title":"ì§ì—…","text":string}],"recommendations":string[],"disclaimer":string}`;

const USER = `ì‚¬ìš©ì: {"name":"ê¹€ì„œì¤€","birthDate":"1990-05-15","gender":"male","calendarType":"solar"}
ìƒí’ˆ: standard
ê°„ë‹¨í•œ ë¬´ë£Œ í”„ë¦¬ë·° ìˆ˜ì¤€(2ê°œ ì„¹ì…˜ë§Œ)ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.`;

// â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

type CompareResult = {
  model: string;
  provider: string;
  durationMs: number;
  inputTokens: number;
  outputTokens: number;
  charCount: number;
  estimatedCostUsd: number;
  jsonValid: boolean;
  cacheHit?: boolean;
  error?: string;
};

async function main() {
  console.log("=== LLM 8-Model Comparison (w/ JSON mode + prompt caching) ===\n");

  const results: CompareResult[] = [];

  for (const def of MODELS) {
    const envKey = def.provider === "openai" ? "OPENAI_API_KEY"
      : def.provider === "anthropic" ? "ANTHROPIC_API_KEY"
      : "GOOGLE_API_KEY";

    if (!process.env[envKey]) {
      console.log(`â­ï¸  ${def.id} â€” skipped (no ${envKey})`);
      results.push({
        model: def.id, provider: def.provider,
        durationMs: 0, inputTokens: 0, outputTokens: 0,
        charCount: 0, estimatedCostUsd: 0, jsonValid: false,
        error: `Missing ${envKey}`,
      });
      continue;
    }

    process.stdout.write(`ğŸ”„ ${def.id}... `);
    const start = Date.now();

    try {
      const result = await callModel(def, SYSTEM, USER, 2000);
      const durationMs = Date.now() - start;

      let jsonValid = false;
      try {
        const parsed = JSON.parse(result.text);
        jsonValid = !!(parsed.headline && parsed.sections);
      } catch { /* not valid JSON */ }

      const costUsd =
        (result.inputTokens / 1000) * def.costPer1kInput +
        (result.outputTokens / 1000) * def.costPer1kOutput;

      const entry: CompareResult = {
        model: def.id, provider: def.provider,
        durationMs, inputTokens: result.inputTokens, outputTokens: result.outputTokens,
        charCount: result.text.length, estimatedCostUsd: costUsd,
        jsonValid, cacheHit: result.cacheHit,
      };
      results.push(entry);

      const cacheTag = result.cacheHit ? " [CACHE]" : "";
      console.log(
        `âœ… ${durationMs}ms | ${result.inputTokens}+${result.outputTokens} tok | ` +
        `${result.text.length}ì | $${costUsd.toFixed(6)} | JSON:${jsonValid ? "OK" : "FAIL"}${cacheTag}`
      );
    } catch (err: any) {
      const durationMs = Date.now() - start;
      console.log(`âŒ ${durationMs}ms | ${err.message?.slice(0, 80)}`);
      results.push({
        model: def.id, provider: def.provider,
        durationMs, inputTokens: 0, outputTokens: 0,
        charCount: 0, estimatedCostUsd: 0, jsonValid: false,
        error: err.message?.slice(0, 120),
      });
    }
  }

  // â”€â”€ Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  console.log("\n=== Summary ===\n");
  console.log(
    "Model".padEnd(30) +
    "Time(ms)".padStart(10) +
    "In-tok".padStart(10) +
    "Out-tok".padStart(10) +
    "Chars".padStart(8) +
    "Cost($)".padStart(12) +
    "JSON".padStart(6)
  );
  console.log("-".repeat(86));

  for (const r of results) {
    if (r.error) {
      console.log(`${r.model.padEnd(30)}${"SKIP/ERR".padStart(10)}  ${r.error}`);
    } else {
      console.log(
        r.model.padEnd(30) +
        String(r.durationMs).padStart(10) +
        String(r.inputTokens).padStart(10) +
        String(r.outputTokens).padStart(10) +
        String(r.charCount).padStart(8) +
        `$${r.estimatedCostUsd.toFixed(6)}`.padStart(12) +
        (r.jsonValid ? "  âœ…" : "  âŒ").padStart(6)
      );
    }
  }

  // â”€â”€ Recommendation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  const valid = results.filter(r => r.jsonValid && !r.error);
  if (valid.length > 0) {
    const cheapest = valid.reduce((a, b) => a.estimatedCostUsd < b.estimatedCostUsd ? a : b);
    const fastest = valid.reduce((a, b) => a.durationMs < b.durationMs ? a : b);
    const longest = valid.reduce((a, b) => a.charCount > b.charCount ? a : b);

    console.log("\nğŸ“Š Recommendation:");
    console.log(`  ğŸ’° Cheapest: ${cheapest.model} ($${cheapest.estimatedCostUsd.toFixed(6)}/req)`);
    console.log(`  âš¡ Fastest:  ${fastest.model} (${fastest.durationMs}ms)`);
    console.log(`  ğŸ“ Longest:  ${longest.model} (${longest.charCount}ì)`);
  }
}

main().catch(console.error);
